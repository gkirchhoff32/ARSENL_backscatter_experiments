{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "313e0f7f",
   "metadata": {},
   "source": [
    "# Fitting to time tag data natively\n",
    "\n",
    "This notebook is intended to give an example of how to fit an estimated photon arrival rate to time tags (without binning).  \n",
    "\n",
    "This notebook uses simulated data which.  To generate this data, you will need a function in the repo\n",
    "\n",
    "    NCAR/deadtime-sim-ptv\n",
    "    \n",
    "This notebook utilizes PyTorch to perform the fitting routine.\n",
    "\n",
    "Our aim is to obtain an estimate of $\\lambda(t)$, the photon arrival rate, as a function of time $t$ (relative to laser pulse firing).  We obtain this using a list of $N$ time tags where photons arrived $\\mathbf{t} =  \\lbrace t_n \\rbrace_{n=1}^N$.\n",
    "\n",
    "For a Poisson point process the probability that the random photon arrivals $\\mathbf{T}$ correspond the observations is\n",
    "\n",
    "$\\mathbb{P}\\left( \\lbrace T_n = t_n \\rbrace_{n=1}^N \\right) = e^{-\\Lambda(t_{max})}\\prod_{n=1}^{N} \\lambda(t_n)$\n",
    "\n",
    "where\n",
    "\n",
    "$\\Lambda(t) = \\int_0^{t} \\lambda(t') dt'$\n",
    "\n",
    "and $t_{max}$ is the maximum time in the retrieval.\n",
    "\n",
    "The above probability accounts for one laser shot.  For multiple shots, the PDFs should be multiplied.  In that case, for $L$ laser shots we obtain\n",
    "\n",
    "$\\mathbb{P}\\left( \\lbrace T_n = t_n \\rbrace_{n=1}^N \\right) = e^{-L \\Lambda(t_{max})}\\prod_{n=1}^{N} \\lambda(t_n)$\n",
    "\n",
    "where the list of photons comprises all arrivals from all laser shots.\n",
    "\n",
    "In order to perform a fit, we aim to minimize the negative log-likelihood of the PDF.  This gives the loss function\n",
    "\n",
    "$\\mathcal{L}[\\lambda(t)] = L \\Lambda(t_{max}) - \\sum_{n=1}^{N} \\ln\\lambda(t_n)$\n",
    "\n",
    "From the above definition, it becomes clear that it is useful to have basis functions for $\\lambda(t)$ that can be integrated analytically.  For this example, we use a Gaussian target in a constant background\n",
    "\n",
    "$\\lambda(t) = A \\exp\\left( -\\frac{(t-\\mu)^2}{2\\sigma^2}\\right) + b$\n",
    "\n",
    "where the parameters $A$, $\\mu$, $\\sigma$ and $b$ must be estimated in the fitting routine.  The integral of the above arrival rate basis function is\n",
    "\n",
    "$\\Lambda(t) = \\frac{1}{2} A \\sigma \\sqrt{2\\pi} \\left[1 + erf\\left(\\frac{t-\\mu}{\\sigma\\sqrt{2}}\\right)\\right] + b t$\n",
    "\n",
    "where $erf$ is the Gauss Error Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b15e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float64\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa200cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library for simulating Poisson point processes\n",
    "dirP_str = os.path.join(os.environ['HOME'], \n",
    "                    'Documents',\n",
    "                    'GitHub',\n",
    "                    'deadtime-sim-ptv',\n",
    "                    'python',\n",
    "                    'library')\n",
    "if dirP_str not in sys.path:\n",
    "    sys.path.append(dirP_str)\n",
    "    \n",
    "import sim_deadtime_utils as sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c8094",
   "metadata": {},
   "source": [
    "# Generate Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e710c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation resolution settings\n",
    "t_sim_min = 0\n",
    "t_sim_max = 1e-6\n",
    "dt_sim = 1e-12\n",
    "\n",
    "tD = 200e-9 # deadtime\n",
    "Nshot = 500 # number of laser shots\n",
    "wrap_deadtime = True  # wrap deadtime between shots\n",
    "\n",
    "laser_pulse_width = 20e-9 # laser pulse width in seconds\n",
    "target_time = 700e-9\n",
    "target_amplitude = 5e6 # target peak count rate\n",
    "background = 1e4  # background count rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the simulated scene time axis\n",
    "t_sim = np.arange(t_sim_min,t_sim_max,dt_sim)           # simulation time\n",
    "t_sim_bins = np.concatenate((t_sim,t_sim[-1:]+dt_sim))  # simulation time histogram bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c197fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the photon arrival rate of the profile\n",
    "# Gaussian target with constant background\n",
    "photon_rate_arr = target_amplitude*np.exp(-(t_sim - target_time)**2/(2*laser_pulse_width**2))+background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab16ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate photon counts\n",
    "\n",
    "# lists of photon arrivals per\n",
    "# laser shot\n",
    "t_det_lst = []  # detected photons (includes deadtime)\n",
    "t_phot_lst = [] # actual photons (no dead time)\n",
    "\n",
    "t_det_last = -100.0  # last photon detection event\n",
    "for n in range(Nshot):\n",
    "    # simulate a laser shot\n",
    "    ptime,ctime = sim.photon_count_generator(t_sim_bins, \n",
    "                                            photon_rate_arr, \n",
    "                                            tau_d_flt=tD,\n",
    "                                            last_photon_flt=t_det_last)\n",
    "    if wrap_deadtime:\n",
    "        if len(ctime) > 0:\n",
    "            t_det_last = ctime[-1]\n",
    "        t_det_last -= t_sim_bins[-1]\n",
    "        \n",
    "    t_det_lst += [ctime]  # detection time tags (including deadtime)\n",
    "    t_phot_lst += [ptime] # photon time tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b83895",
   "metadata": {},
   "outputs": [],
   "source": [
    "phot_arr = np.array(sorted(np.concatenate(t_phot_lst)))\n",
    "plt.figure()\n",
    "plt.stem(phot_arr,np.ones(phot_arr.size))\n",
    "plt.title('Photons')\n",
    "\n",
    "\n",
    "cnt_arr = np.array(sorted(np.concatenate(t_det_lst)))\n",
    "plt.figure()\n",
    "plt.stem(cnt_arr,np.ones(cnt_arr.size))\n",
    "plt.title('Detected Photons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a0c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into fit and validation sets\n",
    "# since we are assuming a fixed target between shots\n",
    "# we can just split the data in chunks.\n",
    "split_index = int(len(t_phot_lst)//2)\n",
    "cnt_phot_fit = len(t_phot_lst[:split_index])\n",
    "cnt_phot_val = len(t_phot_lst[split_index:])\n",
    "t_phot_fit = np.concatenate(t_phot_lst[:split_index])\n",
    "t_phot_val = np.concatenate(t_phot_lst[split_index:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b3a1c",
   "metadata": {},
   "source": [
    "Double check the integral approximation using erf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_torch = torch.tensor(t_sim)\n",
    "integral_est = torch.tensor(t_sim*background) \\\n",
    "        + 0.5*target_amplitude*laser_pulse_width*np.sqrt(2*np.pi) \\\n",
    "        + 0.5*target_amplitude*laser_pulse_width*np.sqrt(2*np.pi)\\\n",
    "            *torch.special.erf((t_torch-target_time)/(laser_pulse_width*np.sqrt(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4997588",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(t_sim,photon_rate_arr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t_sim,np.cumsum(photon_rate_arr)*dt_sim)\n",
    "plt.plot(t_sim,integral_est.numpy(),'--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10467cb0",
   "metadata": {},
   "source": [
    "# Perform Fit Using PyTorch\n",
    "Assume the target is described by a Gaussian with a constant background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d77b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert training and validation data to \n",
    "# tensor type\n",
    "t_phot_fit_tnsr = torch.tensor(t_phot_fit)\n",
    "t_phot_val_tnsr = torch.tensor(t_phot_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9f7348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the fit model as a NN module\n",
    "class Fit_Pulse(torch.nn.Module):\n",
    "    def __init__(self,t_max):\n",
    "        \"\"\"\n",
    "        Instantiate and initialize the fit parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.log_A = torch.nn.Parameter(5*torch.ones(()))   # Gaussian amplitude\n",
    "        self.log_mu = torch.nn.Parameter(-9*torch.ones(()))  # Gaussian mean\n",
    "        self.log_sig = torch.nn.Parameter(-9*torch.ones(())) # Gaussian std\n",
    "        self.log_b = torch.nn.Parameter(3*torch.ones(()))      # background\n",
    "        self.t_max = t_max # maximum time evaluated in integral term\n",
    "        \n",
    "        self.sqrt_2pi = torch.sqrt(torch.tensor(2*np.pi))\n",
    "        self.sqrt_2  = torch.sqrt(torch.tensor(2.0))\n",
    "        \n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        Forward model the profile for input time t.\n",
    "        Also return the integral.\n",
    "        \"\"\"\n",
    "        # calculate all the actual fit parameters\n",
    "        A = torch.exp(self.log_A)\n",
    "        mu = torch.exp(self.log_mu)\n",
    "        sig = torch.exp(self.log_sig)\n",
    "        b = torch.exp(self.log_b)\n",
    "        \n",
    "        # calculate the forward model\n",
    "        model_out = A*torch.exp(-(t-mu)**2/(2*sig**2)) + b\n",
    "        \n",
    "        # calculate the integral\n",
    "        integral_out = b*self.t_max \\\n",
    "                        + 0.5*A*sig*self.sqrt_2pi \\\n",
    "                        + 0.5*A*sig*self.sqrt_2pi\\\n",
    "                            *torch.special.erf((self.t_max-mu)/(sig*self.sqrt_2))\n",
    "        \n",
    "        return model_out, integral_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pois_loss(prof,integral):\n",
    "    \"\"\"\n",
    "    Poisson point process loss function\n",
    "    prof: estimated photon arrival rate evaluated at\n",
    "        each time tag\n",
    "    integral: integral of prof at t_max\n",
    "    \"\"\"\n",
    "    return integral-torch.sum(torch.log(prof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83443f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization parameters\n",
    "rel_step_lim = 1e-15  # termination criteria based on step size\n",
    "max_epochs = 5000     # maximum number of iterations/epochs\n",
    "learning_rate = 1e-1  # ADAM learning rate\n",
    "term_persist = 20     # relative step size averaging interval in iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816cb1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize for fit loop\n",
    "fit_model = Fit_Pulse(t_sim[-1])\n",
    "optimizer = torch.optim.Adam(fit_model.parameters(),lr=learning_rate)\n",
    "epoch=0\n",
    "rel_step = 1e3*rel_step_lim\n",
    "train_loss_lst = []\n",
    "valid_loss_lst = []\n",
    "rel_step_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0698975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the loss function to use a \n",
    "# Poisson point process likelihood function\n",
    "loss_fn = pois_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f3c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform fit\n",
    "while rel_step > rel_step_lim and epoch < max_epochs:\n",
    "    fit_model.train()\n",
    "    pred,integral = fit_model(t_phot_fit_tnsr)\n",
    "    loss = loss_fn(pred,integral*cnt_phot_fit)  # add regularization here\n",
    "    train_loss_lst +=[loss.item()]\n",
    "\n",
    "    # calculate relative step as an average over the last\n",
    "    # term_persist iterations\n",
    "    if epoch == 0:\n",
    "        rel_step_lst+=[1e3*rel_step_lim]\n",
    "        rel_step = 1e3*rel_step_lim\n",
    "    else:\n",
    "        rel_step_lst+=[(train_loss_lst[-2]-train_loss_lst[-1])/np.abs(train_loss_lst[-2])]\n",
    "        rel_step = np.abs(np.array(rel_step_lst)[-term_persist:].mean())\n",
    "    \n",
    "    # calculate validation loss... for fun\n",
    "    pred_val,integral_val = fit_model(t_phot_val_tnsr)\n",
    "    loss_val = loss_fn(pred_val,integral_val*cnt_phot_val)\n",
    "    valid_loss_lst +=[loss_val.item()]\n",
    "\n",
    "    # update estimated parameters\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # zero out the gradient for the next step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "#     print(f\"{train_loss_lst[-1]}\")\n",
    "    \n",
    "    epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the validation loss.\n",
    "# we would use this to determine optimimal tuning parameters\n",
    "# if we had any (e.g. more Gaussian targets)\n",
    "pred_val,integral_val = fit_model(t_phot_val_tnsr)\n",
    "loss_val = loss_fn(pred_val,integral_val*cnt_phot_val)\n",
    "print(f\"Validation Loss: {loss_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7000c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss over iterations\n",
    "plt.figure()\n",
    "plt.plot(train_loss_lst,label='fit loss')\n",
    "plt.plot(valid_loss_lst,label='validation loss')\n",
    "plt.xlabel('fit iteration')\n",
    "plt.ylabel('Poisson NLL')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b755d",
   "metadata": {},
   "source": [
    "Display the resulting estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa55637",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mod_full,integral_full = fit_model(torch.tensor(t_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(t_sim*1e9,photon_rate_arr*1e-6,label='Actual')\n",
    "plt.plot(t_sim*1e9,pred_mod_full.detach().numpy()*1e-6,'--',label='Fit') # /cnt_phot_fit\n",
    "plt.xlabel('time [ns]')\n",
    "plt.ylabel('Photon Arrival Rate [MHz]')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9df353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the modeled values at each photon arrival\n",
    "pred_mod_fit_pts,integral_fit_pts = fit_model(t_phot_fit_tnsr)\n",
    "pred_mod_val_pts,integral_val_pts = fit_model(t_phot_val_tnsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(t_sim*1e9,photon_rate_arr*1e-6,label='Actual',color='k',alpha=0.3)\n",
    "plt.scatter(t_phot_fit_tnsr.detach().numpy()*1e9,pred_mod_fit_pts.detach().numpy()*1e-6,s=2,label='Fit')\n",
    "plt.scatter(t_phot_val_tnsr.detach().numpy()*1e9,pred_mod_val_pts.detach().numpy()*1e-6,s=2,label='Validation',c='tab:red')\n",
    "plt.xlabel('time [ns]')\n",
    "plt.ylabel('Photon Arrival Rate [MHz]')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd0205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c51b55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arsenl_backscatter",
   "language": "python",
   "name": "arsenl_backscatter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
